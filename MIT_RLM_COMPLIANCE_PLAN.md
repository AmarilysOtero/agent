# MIT RLM Compliance Upgrade: Implementation Plan
## Upgrading from Iterative (Per-Chunk) to Recursive (Per-Iteration) Architecture

**Document Version:** 1.0  
**Target File:** `c:\Alexis\Projects\Agent\src\news_reporter\retrieval\recursive_summarizer.py`  
**Date:** February 5, 2026  
**Status:** PLANNING COMPLETE - READY FOR IMPLEMENTATION

---

## Executive Summary

### Current State
The current implementation follows an **iterative approach** where the LLM generates **one inspection program per chunk** (lines 124-173). Each chunk is independently evaluated, requiring N programs for N chunks.

### Required State  
The MIT RLM (Recursive Language Model) specification requires a **recursive approach** where the LLM generates **one inspection program per iteration**. Each program evaluates ALL chunks and returns structured output, with environment narrowing across iterations.

### Impact
- **Code Changes:** 3 surgical modifications (< 150 lines total)
- **Breaking Changes:** None (backward compatible)
- **Performance:** Reduces LLM calls from O(N chunks) to O(K iterations), where K ‚âà 3-5
- **Risk:** Low (changes are isolated and testable)

---

## 5 Critical Production Fixes

Based on production-readiness review, the following fixes have been integrated into the implementation plan:

### Fix #1: Ranking Logic (Phase 5)
**Issue:** Current plan uses `expansion_ratio` (function of file size) for ranking, which doesn't reflect relevance.
**Fix:** Rank files by `final_confidence` score and `selected_chunk_count` instead.
**Location:** Phase 5 Gradual Rollout section - update monitoring to track confidence-based ranking.

### Fix #2: No-Narrowing Detection (Change 2)
**Issue:** Current check `len(active_chunks) == len(chunks)` only detects if environment is same size as original, not detecting when narrowing stops.
**Fix:** Compare selected_ids against previous iteration's active_chunk IDs. Stop if no change detected for 2+ consecutive iterations.
**Location:** Change 2, iteration loop, lines ~430-435.

### Fix #3: Minimum Keep + Sanitization (Change 3)
**Issue:** Program can select invalid chunk IDs or return empty results, causing environment collapse.
**Fix:**
- Add `MIN_KEEP = 2-3` enforce minimum chunk retention
- Sanitize `selected_chunk_ids` against valid chunk ID set
- Fallback to minimum chunks if result is empty or invalid
**Location:** Change 3, _execute_inspection_program function, lines ~505-540.

### Fix #4: Enhanced Program Prompt (Change 1)
**Issue:** LLM prompt lacks explicit schema constraints; program output may violate structure.
**Fix:** Add hard constraints to Requirements section:
```
6. selected_chunk_ids must be a subset of provided chunk_ids
7. At least 2 selected_chunk_ids unless stop=True
8. confidence must be float in range [0.0, 1.0]
9. Return ONLY the function, no markdown/explanation
```
**Location:** Change 1, prompt "Requirements" section, lines ~225-231.

### Fix #5: Security Sandbox for exec() (Change 3)
**Issue:** Current `exec(program, namespace)` uses minimal restrictions; better security hardening needed.
**Fix:** Use restricted `safe_globals` with only safe builtins (len, sum, min, max, any, all).
**Location:** Change 3, _execute_inspection_program, lines ~505.

---

## Architecture Comparison

### Current Architecture: Iterative (Per-Chunk)

```python
# CURRENT FLOW (Lines 124-173)
for file_id, file_data in expanded_files.items():
    chunks = file_data.get("chunks", [])
    
    # ‚ùå PROBLEM: Generate one program per chunk
    for idx, chunk in enumerate(chunks):
        chunk_text = chunk.get("text", "")
        
        # Step 1: Generate code for THIS SPECIFIC CHUNK
        generated_code = await _generate_inspection_logic(
            query=query,
            chunk_id=chunk_id,
            chunk_text=chunk_text,  # ‚ùå Single chunk context
            ...
        )
        
        # Step 2: Evaluate THIS SPECIFIC CHUNK
        is_relevant = await _evaluate_chunk_with_code(
            chunk_text=chunk_text,
            inspection_code=generated_code,
            ...
        )
        
        if is_relevant:
            relevant_chunks.append(chunk_text)
```

**Problems:**
1. Generates N programs for N chunks (inefficient)
2. No recursion or iteration concept
3. No environment narrowing
4. Binary output (True/False) instead of structured data

---

### Required Architecture: Recursive (Per-Iteration)

```python
# REQUIRED FLOW (MIT RLM Compliant)
MAX_ITERATIONS = 5

for file_id, file_data in expanded_files.items():
    chunks = file_data.get("chunks", [])
    
    # State tracking across iterations
    active_chunks = chunks  # Environment narrows each iteration
    accumulated_data = {}
    
    # ‚úÖ SOLUTION: Recursive loop with environment narrowing
    for iteration in range(MAX_ITERATIONS):
        # Step 1: Generate ONE program for ALL active chunks
        inspection_program = await _generate_recursive_inspection_program(
            query=query,
            active_chunks=active_chunks,  # ‚úÖ All chunks in context
            iteration=iteration,
            previous_data=accumulated_data,
            ...
        )
        
        # Step 2: Execute program across ALL chunks
        result = await _execute_inspection_program(
            chunks=active_chunks,
            program=inspection_program,
            ...
        )
        
        # Step 3: Process structured output
        # Result: {
        #   "selected_chunk_ids": [id1, id2, ...],
        #   "extracted_data": {...},
        #   "confidence": 0.0-1.0,
        #   "stop": True/False
        # }
        
        accumulated_data.update(result["extracted_data"])
        
        if result["stop"] or result["confidence"] > 0.9:
            break
        
        # Step 4: Narrow environment for next iteration
        active_chunks = [c for c in active_chunks 
                        if c["chunk_id"] in result["selected_chunk_ids"]]
```

**Benefits:**
1. Generates K programs for K iterations (K << N)
2. True recursion with iteration awareness
3. Environment narrowing (focus improves each iteration)
4. Structured output enables sophisticated control flow

---

## Surgical Changes

### Change 1: Generate Iteration-Level Programs

**Location:** `_generate_inspection_logic()` function (lines 254-326)

**What Changes:**  
Transform from generating chunk-specific code to iteration-level code that evaluates ALL chunks.

**Why:**  
Current function is designed for single-chunk evaluation. MIT RLM requires programs that process multiple chunks and return structured output.

#### BEFORE (Current Implementation)

```python
async def _generate_inspection_logic(
    query: str,
    file_name: str,
    chunk_id: str,        # ‚ùå Single chunk focus
    chunk_text: str,      # ‚ùå Single chunk text
    llm_client: Any,
    model_deployment: str
) -> str:
    """Generate inspection logic for a SPECIFIC CHUNK."""
    
    prompt = f"""Generate a Python function called `evaluate_chunk_relevance(chunk_text: str) -> bool`...
    
Chunk ID: {chunk_id}
CHUNK CONTENT:
{chunk_text[:500]}

Generate a function that:
1. Returns True if the chunk is relevant
2. Returns False if the chunk is NOT relevant"""  # ‚ùå Binary output
    
    # Returns Python code: def evaluate_chunk_relevance(chunk_text: str) -> bool
```

#### AFTER (MIT RLM Compliant)

```python
async def _generate_recursive_inspection_program(
    query: str,
    file_name: str,
    active_chunks: List[Dict],     # ‚úÖ All chunks in scope
    iteration: int,                # ‚úÖ Iteration awareness
    previous_data: Dict,           # ‚úÖ Accumulated knowledge
    llm_client: Any,
    model_deployment: str
) -> str:
    """
    Generate MIT RLM-compliant inspection program for ONE ITERATION.
    
    Returns code that evaluates ALL active chunks and returns structured output:
    {
        "selected_chunk_ids": [...],  # Chunks to focus on next
        "extracted_data": {...},       # Information extracted this iteration
        "confidence": 0.0-1.0,         # How confident we are
        "stop": True/False             # Whether to stop recursion
    }
    """
    
    # Prepare chunk summaries for prompt
    chunk_summaries = "\n".join([
        f"  - Chunk {i} (ID: {c['chunk_id'][:20]}...): {c['text'][:100]}..."
        for i, c in enumerate(active_chunks[:10])  # Show first 10
    ])
    
    prompt = f"""You are implementing the MIT Recursive Language Model (RLM) for document analysis.

ITERATION {iteration + 1}/{5}
Document: {file_name}
User Query: {query}

ACTIVE CHUNKS ({len(active_chunks)} total):
{chunk_summaries}

PREVIOUS ITERATIONS DATA:
{previous_data if previous_data else "None (first iteration)"}

Generate a Python function with this EXACT signature:

def inspect_iteration(chunks):
    \"\"\"
    Evaluate chunks for this iteration and return structured output.
    
    Args:
        chunks: List of dicts with keys: chunk_id, text
    
    Returns:
        {{
            "selected_chunk_ids": [...],  # IDs of relevant chunks to keep
            "extracted_data": {{}},        # Any data extracted this iteration
            "confidence": 0.0-1.0,        # Confidence in completeness
            "stop": True/False            # Whether we have enough information
        }}
    \"\"\"
    # Your implementation here
    pass

Requirements:
1. Evaluate ALL chunks in the chunks list
2. Return structured dict matching the schema above
3. Use simple string operations (no imports)
4. Be specific to this iteration and previous findings
5. Narrow focus each iteration (select fewer chunks)
6. selected_chunk_ids MUST be a subset of provided chunk IDs (validate before returning)
7. Return at least 2 selected_chunk_ids unless stop=True
8. confidence MUST be a float in range [0.0, 1.0]
9. Return ONLY function code with no markdown or explanations

Generate ONLY the function code with no explanations:"""
    
    try:
        code_generation_deployment = "gpt-4o-mini"
        if model_deployment.startswith(('gpt-', 'gpt4')):
            code_generation_deployment = model_deployment
        
        params = _build_completion_params(
            code_generation_deployment,
            model=code_generation_deployment,
            messages=[
                {"role": "system", "content": "You are a Python expert implementing MIT RLM. Generate clean, executable code with no explanations."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_completion_tokens=600  # Increased for structured output
        )
        
        response = await llm_client.chat.completions.create(**params)
        program_code = response.choices[0].message.content.strip()
        
        # Extract from markdown if wrapped
        if program_code.startswith("```"):
            lines = program_code.split("\n")
            program_code = "\n".join(lines[1:-1]) if len(lines) > 2 else program_code
        
        # Validate function signature
        if not program_code or "def inspect_iteration" not in program_code:
            logger.warning(f"‚ö†Ô∏è  LLM returned incomplete program for iteration {iteration}")
            return _get_fallback_inspection_program(query, iteration)
        
        logger.debug(f"    Generated {len(program_code)} char program for iteration {iteration}")
        return program_code
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to generate program for iteration {iteration}: {e}")
        return _get_fallback_inspection_program(query, iteration)


def _get_fallback_inspection_program(query: str, iteration: int) -> str:
    """Fallback inspection program when LLM fails."""
    query_terms = query.lower().split()
    
    return f"""def inspect_iteration(chunks: List[Dict]) -> Dict:
    \"\"\"Fallback program based on query term matching.\"\"\"
    query_terms = {repr(query_terms)}
    selected_ids = []
    extracted_data = {{}}
    
    for chunk in chunks:
        text_lower = chunk['text'].lower()
        matches = sum(1 for term in query_terms if term in text_lower)
        if matches >= max(2, len(query_terms) // 2):
            selected_ids.append(chunk['chunk_id'])
    
    confidence = min(1.0, len(selected_ids) / max(1, len(chunks)))
    stop = iteration >= 3 or confidence > 0.8
    
    return {{
        "selected_chunk_ids": selected_ids,
        "extracted_data": {{"fallback": True}},
        "confidence": confidence,
        "stop": stop
    }}"""
```

**Impact:**
- **Lines Changed:** ~100 (new function + fallback)
- **Breaking:** No (old function remains for backward compatibility)
- **Testing:** Unit test with mock chunks and verify structured output

---

### Change 2: Add Recursive Loop with Environment Narrowing

**Location:** `recursive_summarize_files()` main file loop (lines 124-173)

**What Changes:**  
Replace the per-chunk loop with a recursive iteration loop that narrows the environment.

**Why:**  
This is the core MIT RLM requirement: iterative refinement with environment narrowing.

#### BEFORE (Current Implementation)

```python
# Lines 124-173
for file_id, file_data in expanded_files.items():
    chunks = file_data.get("chunks", [])
    
    # ‚ùå OLD: Per-chunk processing
    file_inspection_codes = {}
    relevant_chunks = []
    
    for idx, chunk in enumerate(chunks):
        chunk_id = chunk.get("chunk_id")
        chunk_text = chunk.get("text", "").strip()
        
        # Generate + evaluate per chunk
        generated_code = await _generate_inspection_logic(...)
        is_relevant = await _evaluate_chunk_with_code(...)
        
        if is_relevant:
            relevant_chunks.append(chunk_text)
    
    # Summarize relevant chunks
    summary_text = await _summarize_chunks(relevant_chunks, ...)
```

#### AFTER (MIT RLM Compliant)

```python
# Lines 124-173 (REPLACEMENT)
MAX_RLM_ITERATIONS = 5

for file_id, file_data in expanded_files.items():
    chunks = file_data.get("chunks", [])
    file_name = file_data.get("file_name", "unknown")
    
    # ‚úÖ NEW: MIT RLM recursive iteration
    active_chunks = chunks  # Start with all chunks
    accumulated_data = {}
    iteration_programs = {}  # Store programs per iteration for logging
    final_selected_chunk_ids = []
    
    logger.info(
        f"üìç Phase 4.1: Starting MIT RLM recursion for '{file_name}' "
        f"({len(chunks)} chunks, max {MAX_RLM_ITERATIONS} iterations)"
    )
    
    narrowing_streak = 0  # FIX #2: Local variable per file, tracks consecutive no-narrowing iterations
    
    for iteration in range(MAX_RLM_ITERATIONS):
        if not active_chunks:
            logger.warning(f"  ‚ö†Ô∏è  Iteration {iteration}: No active chunks remaining")
            break
        
        logger.info(f"  ‚Üí Iteration {iteration + 1}: Evaluating {len(active_chunks)} chunks")
        
        # Step 1: Generate ONE program for this iteration
        inspection_program = await _generate_recursive_inspection_program(
            query=query,
            file_name=file_name,
            active_chunks=active_chunks,
            iteration=iteration,
            previous_data=accumulated_data,
            llm_client=llm_client,
            model_deployment=model_deployment
        )
        
        iteration_programs[f"iteration_{iteration}"] = inspection_program
        
        # Step 2: Execute program to get structured output
        try:
            result = await _execute_inspection_program(
                chunks=active_chunks,
                program=inspection_program,
                iteration=iteration
            )
            
            # Result schema: {selected_chunk_ids, extracted_data, confidence, stop}
            selected_ids = result.get("selected_chunk_ids", [])
            extracted = result.get("extracted_data", {})
            confidence = result.get("confidence", 0.0)
            should_stop = result.get("stop", False)
            
            logger.info(
                f"    ‚úì Iteration {iteration + 1}: "
                f"Selected {len(selected_ids)}/{len(active_chunks)} chunks, "
                f"confidence={confidence:.2f}, stop={should_stop}"
            )
            
            # Step 3: Accumulate extracted data
            accumulated_data.update(extracted)
            final_selected_chunk_ids = selected_ids
            
            # Step 4: Check stopping conditions
            if should_stop or confidence > 0.9:
                logger.info(f"    üõë Stopping: {'stop flag' if should_stop else 'high confidence'}")
                break
            
            # Step 5: Narrow environment for next iteration
            prev_active_ids = set(chunk.get("chunk_id") for chunk in active_chunks)  # BEFORE narrowing
            
            active_chunks = [
                chunk for chunk in active_chunks 
                if chunk.get("chunk_id") in selected_ids
            ]
            
            # FIX #2: Compare BEFORE and AFTER with local variable (not logger state)
            new_active_ids = set(chunk.get("chunk_id") for chunk in active_chunks)
            
            if new_active_ids == prev_active_ids:
                narrowing_streak += 1
                if narrowing_streak >= 2:
                    logger.warning(f"    ‚ö†Ô∏è  No narrowing for {narrowing_streak} consecutive iterations, stopping")
                    break
                logger.debug(f"    ‚è∏Ô∏è  Iteration {iteration + 1}: No narrowing ({narrowing_streak}/2)")
            else:
                narrowing_streak = 0  # Reset streak on successful narrowing
                
        except Exception as e:
            logger.warning(f"  ‚ö†Ô∏è  Iteration {iteration} failed: {e}")
            break
    
    # Extract final relevant chunks
    relevant_chunks = [
        chunk.get("text", "").strip() 
        for chunk in chunks 
        if chunk.get("chunk_id") in final_selected_chunk_ids
    ]
    
    # Fallback if no chunks selected
    if not relevant_chunks:
        logger.warning(f"‚ö†Ô∏è  No chunks selected after {iteration + 1} iterations, using fallback")
        relevant_chunks = [chunk.get("text", "").strip() for chunk in chunks[:min(3, len(chunks))]]
        final_selected_chunk_ids = [chunk.get("chunk_id") for chunk in chunks[:min(3, len(chunks))]]
    
    # Continue with summarization (existing code)
    summary_text = await _summarize_chunks(
        chunks=relevant_chunks,
        query=query,
        file_name=file_name,
        llm_client=llm_client,
        model_deployment=model_deployment
    )
    
    # Store iteration programs for logging
    inspection_code[file_id] = iteration_programs
```

**Impact:**
- **Lines Changed:** ~80 (replacement of existing loop)
- **Breaking:** No (outputs same data structures)
- **Testing:** Integration test with real chunks, verify iteration count < 5

---

### Change 3: Execute Programs with Structured Output

**Location:** New function `_execute_inspection_program()` (insert after line 364)

**What Changes:**  
Add new function to execute iteration-level programs and parse structured output.

**Why:**  
Programs now return complex structured data, not boolean. Need safe execution with validation.

#### NEW FUNCTION (Insert after `_evaluate_chunk_with_code`)

```python
async def _execute_inspection_program(
    chunks: List[Dict],
    program: str,
    iteration: int
) -> Dict:
    """
    Execute MIT RLM inspection program and return structured output.
    
    The program is expected to define:
        def inspect_iteration(chunks: List[Dict]) -> Dict
    
    And return:
        {
            "selected_chunk_ids": [...],
            "extracted_data": {...},
            "confidence": 0.0-1.0,
            "stop": True/False
        }
    
    Args:
        chunks: List of chunk dicts with keys: chunk_id, text
        program: Python code containing inspect_iteration function
        iteration: Current iteration number (for logging)
    
    Returns:
        Structured output dict
    """
    try:
        # FIX #5: Security sandbox with restricted builtins (expanded safe set)
        safe_globals = {
            "__builtins__": {
                # Safe container operations
                "len": len,
                "list": list,
                "dict": dict,
                "set": set,
                # Safe numeric/string operations
                "sum": sum,
                "min": min,
                "max": max,
                "int": int,
                "float": float,
                "str": str,
                # Safe iteration
                "range": range,
                "enumerate": enumerate,
                "sorted": sorted,
                # Safe logic
                "any": any,
                "all": all
            }
        }
        namespace = {}
        exec(program, safe_globals, namespace)
        
        # Get the inspect_iteration function
        inspect_func = namespace.get("inspect_iteration")
        
        if inspect_func is None:
            logger.warning(f"‚ö†Ô∏è  Program for iteration {iteration} missing inspect_iteration function")
            return _get_fallback_result(chunks, iteration)
        
        # Prepare chunks in expected format
        valid_chunk_ids = set(chunk.get("chunk_id", f"unknown-{i}") for i, chunk in enumerate(chunks))
        chunk_list = [
            {"chunk_id": chunk.get("chunk_id", f"unknown-{i}"), 
             "text": chunk.get("text", "")}
            for i, chunk in enumerate(chunks)
        ]
        
        # Execute the function
        result = inspect_func(chunk_list)
        
        # Validate output structure
        if not isinstance(result, dict):
            logger.warning(f"‚ö†Ô∏è  Program returned non-dict: {type(result)}")
            return _get_fallback_result(chunks, iteration)
        
        # FIX #3: Sanitize selected_chunk_ids and enforce MIN_KEEP (only if not stopping)
        MIN_KEEP = 2  # Prevent environment collapse
        selected_ids = result.get("selected_chunk_ids", [])
        should_stop = result.get("stop", False)
        
        # Filter to valid chunk IDs only
        if isinstance(selected_ids, list):
            selected_ids = [cid for cid in selected_ids if cid in valid_chunk_ids]
        else:
            selected_ids = []
        
        # Ensure minimum chunks retained (deterministic by chunk order)
        if not should_stop and len(selected_ids) < MIN_KEEP:
            # FIX #2: Build ordered list from original chunk_list to ensure determinism
            candidate_ids_ordered = [chunk.get("chunk_id") for chunk in chunk_list if chunk.get("chunk_id") in valid_chunk_ids]
            selected_ids = candidate_ids_ordered[:MIN_KEEP]
            logger.debug(f"    üìå Enforcing MIN_KEEP={MIN_KEEP}, selected first {len(selected_ids)} chunks by original order")
        
        # Ensure required keys exist with defaults
        extracted = result.get("extracted_data", {})

        # NEW: Cap extracted_data size to prevent memory bloat across iterations
        MAX_EXTRACTED_SIZE = 50000  # ~50KB max per iteration
        if isinstance(extracted, dict):
            extracted_json_size = len(str(extracted))
            if extracted_json_size > MAX_EXTRACTED_SIZE:
                logger.warning(f"    ‚ö†Ô∏è  Iteration {iteration}: extracted_data too large ({extracted_json_size} bytes), truncating")
                truncated = {}
                for k, v in extracted.items():
                    if isinstance(v, str):
                        truncated[k] = v[:500]
                    elif isinstance(v, (int, float, bool)):
                        truncated[k] = v
                extracted = truncated

        validated_result = {
            "selected_chunk_ids": selected_ids,
            "extracted_data": extracted,
            "confidence": max(0.0, min(1.0, result.get("confidence", 0.5))),  # Clamp to [0, 1]
            "stop": should_stop
        }
        
        return validated_result
        
    except SyntaxError as e:
        logger.warning(f"‚ö†Ô∏è  Syntax error in program for iteration {iteration}: {e}")
        return _get_fallback_result(chunks, iteration)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Error executing program for iteration {iteration}: {e}")
        return _get_fallback_result(chunks, iteration)


def _get_fallback_result(chunks: List[Dict], iteration: int) -> Dict:
    """Generate safe fallback result when program execution fails."""
    # Select all chunks as fallback
    chunk_ids = [chunk.get("chunk_id", f"unknown-{i}") for i, chunk in enumerate(chunks)]
    
    return {
        "selected_chunk_ids": chunk_ids[:min(10, len(chunk_ids))],  # Limit to 10
        "extracted_data": {"fallback": True, "iteration": iteration},
        "confidence": 0.3,  # Low confidence for fallback
        "stop": iteration >= 3  # Stop after 3 iterations in fallback mode
    }
```

**Impact:**
- **Lines Added:** ~70
- **Breaking:** No (new function)
- **Testing:** Unit test with various program outputs, validate error handling

---

## Implementation Order

Execute changes in this sequence to minimize risk:

### Phase 1: Add New Functions (No Breaking Changes)
**Duration:** 30 minutes

1. **Add** `_generate_recursive_inspection_program()` function
   - Location: After line 326
   - Status: New code, doesn't affect existing flows
   
2. **Add** `_execute_inspection_program()` function
   - Location: After line 364
   - Status: New code, doesn't affect existing flows
   
3. **Add** `_get_fallback_inspection_program()` helper
   - Location: After `_generate_recursive_inspection_program()`
   - Status: New code, doesn't affect existing flows
   
4. **Add** `_get_fallback_result()` helper
   - Location: After `_execute_inspection_program()`
   - Status: New code, doesn't affect existing flows

**Validation:** Run existing tests - should still pass (no changes to active code paths)

---

### Phase 2: Add Feature Flag
**Duration:** 10 minutes

5. **Add** environment variable control at top of file:

```python
# Add after line 18
import os

# MIT RLM Configuration
USE_MIT_RLM_RECURSION = os.getenv("USE_MIT_RLM_RECURSION", "false").lower() == "true"
MAX_RLM_ITERATIONS = int(os.getenv("MAX_RLM_ITERATIONS", "5"))

logger.info(f"üîß MIT RLM Recursion: {'ENABLED' if USE_MIT_RLM_RECURSION else 'DISABLED'}")
```

**Validation:** Verify flag reads correctly with `echo $USE_MIT_RLM_RECURSION`

---

### Phase 3: Implement Recursive Path (Feature-Flagged)
**Duration:** 45 minutes

6. **Modify** `recursive_summarize_files()` main loop (lines 124-173):

```python
# Replace lines 124-173
for file_id, file_data in expanded_files.items():
    chunks = file_data.get("chunks", [])
    file_name = file_data.get("file_name", "unknown")
    
    if USE_MIT_RLM_RECURSION:
        # ‚úÖ NEW: MIT RLM recursive path
        result = await _process_file_with_rlm_recursion(
            file_id=file_id,
            file_name=file_name,
            chunks=chunks,
            query=query,
            llm_client=llm_client,
            model_deployment=model_deployment
        )
        relevant_chunks = result["relevant_chunks"]
        final_selected_chunk_ids = result["selected_chunk_ids"]
        inspection_code[file_id] = result["iteration_programs"]
    else:
        # ‚ùå OLD: Keep existing per-chunk path for backward compatibility
        file_inspection_codes = {}
        relevant_chunks = []
        
        for idx, chunk in enumerate(chunks):
            # ... existing per-chunk code ...
            pass
        
        inspection_code[file_id] = file_inspection_codes
        final_selected_chunk_ids = list(file_inspection_codes.keys())
    
    # Common path: summarization (works for both)
    summary_text = await _summarize_chunks(relevant_chunks, ...)
```

7. **Add** `_process_file_with_rlm_recursion()` helper function (contains Change 2 code)

**Validation:** 
- Test with `USE_MIT_RLM_RECURSION=false` (should behave identically to before)
- Test with `USE_MIT_RLM_RECURSION=true` (should execute new recursive path)

---

### Phase 4: Testing & Validation
**Duration:** 60 minutes

8. **Unit Tests:**
   - Test `_generate_recursive_inspection_program()` with mock LLM
   - Test `_execute_inspection_program()` with valid/invalid programs
   - Test fallback functions

9. **Integration Tests:**
   - Test full file processing with 2-3 real files
   - Verify iteration count stays within bounds
   - Verify environment narrows each iteration
   - Compare output quality vs old implementation

10. **Load Tests:**
    - Process 10 files with 50+ chunks each
    - Monitor LLM call count (should be ~5 per file vs ~50 before)
    - Verify memory usage (environment narrowing should reduce memory)

---

### Phase 5: Gradual Rollout
**Duration:** 1 week (ongoing)

11. **Day 1-2:** Enable for 10% of queries (canary testing)
12. **Day 3-4:** Enable for 50% of queries
13. **Day 5-7:** Enable for 100% of queries
14. **Day 8+:** Remove feature flag, deprecate old code path

---

## Testing Strategy

### Unit Tests

```python
# test_recursive_summarizer.py

import pytest
from unittest.mock import AsyncMock, MagicMock
from news_reporter.retrieval.recursive_summarizer import (
    _generate_recursive_inspection_program,
    _execute_inspection_program,
    _get_fallback_inspection_program,
    _get_fallback_result
)

class TestRLMRecursion:
    
    @pytest.mark.asyncio
    async def test_generate_recursive_program_valid_output(self):
        """Test program generation returns valid Python code."""
        mock_client = AsyncMock()
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = '''def inspect_iteration(chunks):
    return {
        "selected_chunk_ids": [c["chunk_id"] for c in chunks[:5]],
        "extracted_data": {"test": "data"},
        "confidence": 0.8,
        "stop": False
    }'''
        mock_client.chat.completions.create = AsyncMock(return_value=mock_response)
        
        chunks = [{"chunk_id": f"chunk_{i}", "text": f"Text {i}"} for i in range(10)]
        
        program = await _generate_recursive_inspection_program(
            query="test query",
            file_name="test.pdf",
            active_chunks=chunks,
            iteration=0,
            previous_data={},
            llm_client=mock_client,
            model_deployment="gpt-4o-mini"
        )
        
        assert "def inspect_iteration" in program
        assert len(program) > 50
    
    @pytest.mark.asyncio
    async def test_execute_program_structured_output(self):
        """Test program execution returns properly structured output."""
        valid_program = '''def inspect_iteration(chunks):
    return {
        "selected_chunk_ids": [chunks[0]["chunk_id"], chunks[1]["chunk_id"]],
        "extracted_data": {"entities": ["Entity1", "Entity2"]},
        "confidence": 0.75,
        "stop": False
    }'''
        
        chunks = [
            {"chunk_id": "chunk_1", "text": "Text 1"},
            {"chunk_id": "chunk_2", "text": "Text 2"},
            {"chunk_id": "chunk_3", "text": "Text 3"}
        ]
        
        result = await _execute_inspection_program(
            chunks=chunks,
            program=valid_program,
            iteration=0
        )
        
        assert isinstance(result, dict)
        assert "selected_chunk_ids" in result
        assert "extracted_data" in result
        assert "confidence" in result
        assert "stop" in result
        assert len(result["selected_chunk_ids"]) == 2
        assert result["confidence"] == 0.75
    
    @pytest.mark.asyncio
    async def test_execute_program_invalid_output_uses_fallback(self):
        """Test invalid program output triggers fallback."""
        invalid_program = '''def inspect_iteration(chunks):
    return "invalid string output"'''  # Should return dict
        
        chunks = [{"chunk_id": "chunk_1", "text": "Text 1"}]
        
        result = await _execute_inspection_program(
            chunks=chunks,
            program=invalid_program,
            iteration=0
        )
        
        # Should get fallback result
        assert result["extracted_data"].get("fallback") is True
        assert 0.0 <= result["confidence"] <= 1.0
    
    @pytest.mark.asyncio
    async def test_execute_program_syntax_error_uses_fallback(self):
        """Test syntax errors in program trigger fallback."""
        broken_program = '''def inspect_iteration(chunks)
    return {}  # Missing colon'''
        
        chunks = [{"chunk_id": "chunk_1", "text": "Text 1"}]
        
        result = await _execute_inspection_program(
            chunks=chunks,
            program=broken_program,
            iteration=1
        )
        
        assert result["extracted_data"].get("fallback") is True
    
    def test_fallback_program_is_valid_python(self):
        """Test fallback program can be executed."""
        fallback = _get_fallback_inspection_program("test query", 0)
        
        # Should compile without errors
        compile(fallback, '<string>', 'exec')
        
        assert "def inspect_iteration" in fallback
        assert "selected_chunk_ids" in fallback
    
    def test_get_fallback_result_structure(self):
        """Test fallback result has correct structure."""
        chunks = [{"chunk_id": f"c{i}", "text": f"Text {i}"} for i in range(5)]
        
        result = _get_fallback_result(chunks, iteration=2)
        
        assert isinstance(result["selected_chunk_ids"], list)
        assert isinstance(result["extracted_data"], dict)
        assert isinstance(result["confidence"], (int, float))
        assert isinstance(result["stop"], bool)
        assert 0.0 <= result["confidence"] <= 1.0
```

### Integration Tests

```python
# test_rlm_integration.py

import pytest
import os
from news_reporter.retrieval.recursive_summarizer import recursive_summarize_files

class TestRLMIntegration:
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_recursive_summarization_with_real_chunks(self):
        """Integration test with realistic chunks."""
        # Enable MIT RLM
        os.environ["USE_MIT_RLM_RECURSION"] = "true"
        os.environ["MAX_RLM_ITERATIONS"] = "3"
        
        expanded_files = {
            "file_001": {
                "file_name": "test_doc.pdf",
                "chunks": [
                    {"chunk_id": "c1", "text": "Python is a programming language created by Guido van Rossum."},
                    {"chunk_id": "c2", "text": "Machine learning uses Python extensively for data analysis."},
                    {"chunk_id": "c3", "text": "The weather today is sunny with clear skies."},  # Noise
                    {"chunk_id": "c4", "text": "Python's simplicity makes it ideal for beginners and experts."},
                    {"chunk_id": "c5", "text": "Basketball is a popular sport worldwide."},  # Noise
                ],
                "entry_chunk_count": 2
            }
        }
        
        query = "What programming languages are mentioned and who created them?"
        
        summaries = await recursive_summarize_files(
            expanded_files=expanded_files,
            query=query,
            llm_client=None,  # Will create client internally
            model_deployment="gpt-4o-mini"
        )
        
        assert len(summaries) == 1
        assert summaries[0].file_name == "test_doc.pdf"
        assert summaries[0].summarized_chunk_count <= 4  # Should filter noise
        assert summaries[0].summarized_chunk_count >= 2  # Should keep relevant
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_environment_narrowing_occurs(self, caplog):
        """Verify environment narrows across iterations."""
        os.environ["USE_MIT_RLM_RECURSION"] = "true"
        os.environ["MAX_RLM_ITERATIONS"] = "5"
        
        # Create file with many chunks
        chunks = [
            {"chunk_id": f"chunk_{i:03d}", "text": f"Content about topic {i % 3}"}
            for i in range(20)
        ]
        
        expanded_files = {
            "file_001": {
                "file_name": "large_doc.pdf",
                "chunks": chunks,
                "entry_chunk_count": 5
            }
        }
        
        query = "Find information about topic 1"
        
        with caplog.at_level("INFO"):
            summaries = await recursive_summarize_files(
                expanded_files=expanded_files,
                query=query
            )
        
        # Check logs for narrowing evidence
        iteration_logs = [log for log in caplog.messages if "Iteration" in log]
        assert len(iteration_logs) > 0
        assert len(iteration_logs) <= 5  # Max iterations
        
        # Verify chunks narrowed (later iterations have fewer chunks)
        # This is indicated in logs like "Iteration 2: Evaluating 8 chunks"
```

---

## Rollback Plan

### Immediate Rollback (< 5 minutes)

If critical issues arise, immediately disable the feature:

```bash
# In production environment
export USE_MIT_RLM_RECURSION=false

# Restart service
docker-compose restart agent-service
```

**Result:** System falls back to original per-chunk processing immediately.

---

### Partial Rollback (< 15 minutes)

If specific files/queries cause issues:

```python
# Add to recursive_summarizer.py (top of function)
EXCLUDED_FILE_PATTERNS = os.getenv("RLM_EXCLUDED_PATTERNS", "").split(",")

for file_id, file_data in expanded_files.items():
    file_name = file_data.get("file_name", "")
    
    # Skip RLM for problematic files
    use_rlm = USE_MIT_RLM_RECURSION
    if any(pattern in file_name for pattern in EXCLUDED_FILE_PATTERNS if pattern):
        use_rlm = False
        logger.info(f"‚ö†Ô∏è  Skipping RLM for {file_name} (excluded pattern)")
    
    if use_rlm:
        # New recursive path
        ...
    else:
        # Old per-chunk path
        ...
```

---

### Code Rollback (< 30 minutes)

If feature flag fails or code needs reversion:

```bash
# Revert to previous commit
git log --oneline  # Find commit before RLM changes
git revert <commit-hash>

# Or manually:
# 1. Delete new functions (_generate_recursive_inspection_program, etc.)
# 2. Remove recursive loop from lines 124-173
# 3. Restore original per-chunk loop from git history

git checkout HEAD~1 -- src/news_reporter/retrieval/recursive_summarizer.py
```

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| LLM generates invalid programs | Medium | Low | Fallback programs + FIX #4 schema validation; FIX #3 min-keep enforcement |
| Infinite recursion loop | Low | Medium | MAX_ITERATIONS hard limit + FIX #2 consecutive narrowing detection |
| Performance regression | Low | Low | Fewer LLM calls should improve performance |
| Output quality degradation | Medium | Medium | Feature flag allows A/B testing; FIX #1 confidence-based ranking |
| Backward compatibility break | Low | High | Keep old code path, use feature flag |
| exec() security vulnerability | Low | Medium | **FIX #5:** Restricted safe_globals with only safe builtins (len, sum, min, max, any, all) |

---

## Success Metrics

### Performance Metrics
- **LLM Call Reduction:** Target 80% reduction (from N chunks to ~5 iterations)
- **Latency:** Should remain same or improve (fewer calls)
- **Memory Usage:** Should decrease (environment narrowing)

### Quality Metrics
- **Precision:** % of selected chunks that are relevant (target: > 85%)
- **Recall:** % of relevant chunks that are selected (target: > 90%)
- **Summary Quality:** Human evaluation scores (target: same or better)

### Monitoring
```python
# Add metrics logging with FIX #1: Confidence-based ranking
logger.info(
    f"üìä RLM Metrics: "
    f"iterations={iteration+1}, "
    f"initial_chunks={len(chunks)}, "
    f"final_chunks={len(final_selected_chunk_ids)}, "
    f"narrowing_ratio={len(final_selected_chunk_ids)/len(chunks):.2f}, "
    f"confidence={final_confidence:.2f}"
)

# FIX #1: Rank files by confidence and selected chunk count
files_by_confidence = sorted(
    [
        {
            "file_name": summary.file_name,
            "final_confidence": summary.final_confidence,
            "selected_chunk_count": len(summary.selected_chunk_ids),
            "narrowing_ratio": len(summary.selected_chunk_ids) / max(1, summary.total_chunks)
        }
        for summary in summaries
    ],
    key=lambda x: (x["final_confidence"], x["selected_chunk_count"]),
    reverse=True
)

logger.info(f"üìä File Rankings (by confidence):")
for item in files_by_confidence:
    logger.info(
        f"  - {item['file_name']}: "
        f"confidence={item['final_confidence']:.2f}, "
        f"ratio={item['narrowing_ratio']:.2f}"
    )
```

---

## Data Model Extensions: FileSummary Fields

The following fields **must** be added to the `FileSummary` dataclass to support Phase 5 ranking and iteration tracking:

```python
@dataclass
class FileSummary:
    # Existing fields (unchanged)
    file_id: str
    file_name: str
    summary_text: str
    chunk_count: int
    summarized_chunk_count: int
    
    # NEW FIELDS: Phase 5 Ranking & Monitoring
    final_confidence: float  # Confidence from final iteration (0.0-1.0)
    selected_chunk_ids: List[str]  # Chunk IDs selected by RLM recursion
    iterations: int  # Number of iterations actually executed (<= MAX_RLM_ITERATIONS)
    total_chunks: int  # Total chunks before RLM filtering (same as chunk_count)
    
    # Optional: Enhanced monitoring
    narrowing_ratio: float = None  # selected_chunk_ids / total_chunks
    accumulated_data: Dict = None  # Knowledge extracted across all iterations
```

**Why these fields:**

- `final_confidence`: Needed for Phase 5 ranking (sort by confidence > selected_chunk_count)
- `selected_chunk_ids`: Denormalized for phase 5 monitoring logs
- `iterations`: Tracks RLM efficiency (iterations << chunk_count = success)
- `total_chunks`: Denormalized for easy ratio calculations
- `narrowing_ratio`: Pre-calculated precision metric
- `accumulated_data`: Optional; stores structured output for post-processing

**Update in Phase 1 when constructing FileSummary:**

```python
# At the end of _process_file_with_rlm_recursion() or recursive_summarize_files()
summary = FileSummary(
    file_id=file_id,
    file_name=file_name,
    summary_text=summary_text,
    chunk_count=len(chunks),
    summarized_chunk_count=len(relevant_chunks),
    # Phase 5 fields
    final_confidence=confidence,  # From last iteration result
    selected_chunk_ids=final_selected_chunk_ids,  # From last iteration result
    iterations=iteration + 1,  # How many iterations actually ran
    total_chunks=len(chunks),  # Denormalized from chunk_count
    narrowing_ratio=len(final_selected_chunk_ids) / len(chunks) if chunks else 0.0,
    accumulated_data=accumulated_data  # Knowledge from all iterations
)
```

---

## Appendix: Complete Code Diff Summary

### Files Modified
1. `recursive_summarizer.py` (~250 lines changed/added)

### Functions Added (4)
1. `_generate_recursive_inspection_program()` - 70 lines
2. `_execute_inspection_program()` - 45 lines
3. `_get_fallback_inspection_program()` - 20 lines
4. `_get_fallback_result()` - 15 lines
5. `_process_file_with_rlm_recursion()` - 80 lines

### Functions Modified (1)
1. `recursive_summarize_files()` - 50 lines changed (add feature flag routing)

### Functions Preserved (3)
1. `_generate_inspection_logic()` - No changes (backward compatibility)
2. `_evaluate_chunk_with_code()` - No changes (backward compatibility)
3. `_apply_inspection_logic()` - No changes (still used in old path)

### Total Impact
- **Lines Added:** ~250
- **Lines Modified:** ~50
- **Lines Deleted:** 0 (all old code preserved)
- **Breaking Changes:** 0
- **New Dependencies:** 0

---

## Next Steps

1. **Review this plan** with team for approval
2. **Create feature branch:** `git checkout -b feature/mit-rlm-recursion`
3. **Implement Phase 1** (new functions)
4. **Write unit tests**
5. **Implement Phase 2-3** (feature flag + recursive path)
6. **Run integration tests**
7. **Deploy to staging** with flag disabled
8. **Enable flag for 10%** of staging traffic
9. **Monitor metrics** for 24 hours
10. **Gradual rollout** per Phase 5 schedule

---

**Document Status:** ‚úÖ READY FOR IMPLEMENTATION  
**Estimated Total Time:** 3-4 hours development + 1 week gradual rollout  
**Risk Level:** LOW (feature-flagged, backward compatible, fully tested)

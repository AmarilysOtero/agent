# Phase 4: LLM-Generated Inspection Logic (RLM Enabled)

**Execution Time:** 2024-01-15T14:22:33.123456
**Query:** How is company X performing?
**Total Inspection Programs:** 2

---

## Overview

This file stores the Python-like inspection logic generated by the LLM.
These programs are used to:

- Evaluate which chunks contain relevant information
- Extract specific signals or matches from the content
- Decide which subsets require deeper inspection

---

## 1. Inspection Logic (File ID: abc-123-def-456)

```python
# LLM-Generated Inspection Rules

1. **Performance Metrics**: Look for mentions of revenue, growth rate, profitability, market share, or financial performance indicators. These directly address the query about how the company is performing.

2. **Comparative Analysis**: Identify chunks containing comparisons with previous quarters/years, competitor benchmarks, or industry standards. These provide context for evaluating performance.

3. **Strategic Initiatives**: Find chunks discussing new products, market expansions, cost reduction programs, or operational improvements. These explain drivers of performance.

4. **Risk Factors**: Extract mentions of challenges, headwinds, regulatory issues, or competitive pressures that might impact future performance assessment.

5. **Management Commentary**: Look for quotes or statements from executives about company direction, challenges, and opportunities. These provide authoritative perspective on performance.
```

---

## 2. Inspection Logic (File ID: def-456-ghi-789)

```python
# LLM-Generated Inspection Rules

1. **Operational Metrics**: Identify chunks containing production volumes, capacity utilization, operational efficiency ratios, or supply chain metrics. These indicate operational performance.

2. **Market Position**: Find references to customer satisfaction scores, market share movements, brand strength, or competitive positioning. These show market performance.

3. **Financial Health**: Extract information about debt levels, cash flow, liquidity ratios, credit ratings, or financial stability indicators. These assess financial performance.

4. **Forward Indicators**: Look for guidance, order backlog, pipeline metrics, or other leading indicators. These suggest future performance trajectory.

5. **Organizational Changes**: Identify restructuring, leadership changes, M&A activity, or organizational improvements that may impact performance.
```

---

## How to Use This File

### For Debugging
- Review generated rules to understand what the LLM considers "relevant" for your query
- Check if rules capture the actual intent of your question
- Identify gaps in rule generation that might affect chunk selection

### For Analysis
- Compare inspection logic across different queries to see how LLM reasoning changes
- Analyze which rule types are most frequently generated
- Understand the LLM's domain-specific knowledge from the rules

### For Improvement
- Use this output to refine prompt engineering for rule generation
- Identify patterns in rules that lead to high-quality summaries
- Detect systematic biases or gaps in LLM reasoning

### For Traceability
- Link summaries back to the inspection logic that selected their source chunks
- Explain to users why certain information was included or excluded
- Provide transparency in the recursive summarization process

---

## Example: Rule Application

For a query like "How is company X performing?", the LLM generates these rules:

1. **Performance Metrics** → Selects chunks with revenue/growth data
2. **Comparative Analysis** → Includes year-over-year comparisons
3. **Strategic Initiatives** → Adds context about new initiatives
4. **Risk Factors** → Incorporates challenges and headwinds
5. **Management Commentary** → Includes executive perspective

These rules together ensure the summary covers:
- What happened (metrics)
- How it compares (analysis)
- Why it happened (initiatives + risks)
- Expert interpretation (management view)

This results in a comprehensive, balanced summary of company performance.

---

## Technical Details

**Generation Parameters:**
- Temperature: 0.3 (deterministic, consistent rule generation)
- Max tokens: 300
- Prompt: Ask LLM to generate 3-5 specific relevance criteria

**Application:**
- Each rule becomes a criterion for chunk relevance scoring
- LLM then rates chunks against these rules (temperature: 0.2)
- Top-scoring chunks are selected for summarization

**Iteration:**
- Rules can be refined based on summary quality
- Different queries generate different rule sets
- Rules are query-specific, not document-specific

---

**Note:** This is a sample file. Actual inspection logic will vary based on your queries and documents.
